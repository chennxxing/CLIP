{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-05T09:19:48.588660775Z",
     "start_time": "2023-10-05T09:19:47.819684505Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPModel\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model =  CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T09:19:51.034093580Z",
     "start_time": "2023-10-05T09:19:49.067334869Z"
    }
   },
   "id": "3023c831d96d1250"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                                         std=(0.26862954, 0.26130258, 0.27577711))  # for CLIP\n",
    "val_preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        normalize\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T09:19:51.037695432Z",
     "start_time": "2023-10-05T09:19:51.036244760Z"
    }
   },
   "id": "34f8b167143ce6bf"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "##### data\n",
    "from cifar import Cifar_train, Cifar_test\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T09:19:51.040475160Z",
     "start_time": "2023-10-05T09:19:51.038342421Z"
    }
   },
   "id": "fc3d8019bd1c5f64"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "dataset = Cifar_train(transform=val_preprocess)\n",
    "dataset_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False, num_workers=0, drop_last=True\n",
    "    )\n",
    "\n",
    "dataset_test = Cifar_test(transform=val_preprocess)\n",
    "dataset_loader_test = torch.utils.data.DataLoader(\n",
    "            dataset_test,\n",
    "            batch_size=64,\n",
    "            shuffle=False, num_workers=0, drop_last=True\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T09:19:51.224125814Z",
     "start_time": "2023-10-05T09:19:51.212758040Z"
    }
   },
   "id": "af2a13df8e472790"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.74 GiB total capacity; 14.51 GiB already allocated; 25.44 MiB free; 15.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m CLIP_feature \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m dataset_loader:\n\u001B[0;32m----> 3\u001B[0m     a \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_image_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m      4\u001B[0m     a \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mhstack((a,a))\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m CLIP_feature \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/CLIP/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:1086\u001B[0m, in \u001B[0;36mCLIPModel.get_image_features\u001B[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1081\u001B[0m output_hidden_states \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1082\u001B[0m     output_hidden_states \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39moutput_hidden_states\n\u001B[1;32m   1083\u001B[0m )\n\u001B[1;32m   1084\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1086\u001B[0m vision_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvision_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1087\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1088\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1089\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1090\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1091\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1093\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m vision_outputs[\u001B[38;5;241m1\u001B[39m]  \u001B[38;5;66;03m# pooled_output\u001B[39;00m\n\u001B[1;32m   1094\u001B[0m image_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvisual_projection(pooled_output)\n",
      "File \u001B[0;32m~/anaconda3/envs/CLIP/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/CLIP/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:886\u001B[0m, in \u001B[0;36mCLIPVisionTransformer.forward\u001B[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    883\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(pixel_values)\n\u001B[1;32m    884\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpre_layrnorm(hidden_states)\n\u001B[0;32m--> 886\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    887\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    888\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    889\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    891\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    893\u001B[0m last_hidden_state \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    894\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m last_hidden_state[:, \u001B[38;5;241m0\u001B[39m, :]\n",
      "File \u001B[0;32m~/anaconda3/envs/CLIP/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/CLIP/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:656\u001B[0m, in \u001B[0;36mCLIPEncoder.forward\u001B[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    649\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    650\u001B[0m         create_custom_forward(encoder_layer),\n\u001B[1;32m    651\u001B[0m         hidden_states,\n\u001B[1;32m    652\u001B[0m         attention_mask,\n\u001B[1;32m    653\u001B[0m         causal_attention_mask,\n\u001B[1;32m    654\u001B[0m     )\n\u001B[1;32m    655\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 656\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mencoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    657\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    658\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    659\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcausal_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    660\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    661\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    663\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    665\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/anaconda3/envs/CLIP/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/CLIP/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:396\u001B[0m, in \u001B[0;36mCLIPEncoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001B[0m\n\u001B[1;32m    394\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    395\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm2(hidden_states)\n\u001B[0;32m--> 396\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    397\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    399\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (hidden_states,)\n",
      "File \u001B[0;32m~/anaconda3/envs/CLIP/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/CLIP/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:353\u001B[0m, in \u001B[0;36mCLIPMLP.forward\u001B[0;34m(self, hidden_states)\u001B[0m\n\u001B[1;32m    351\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(hidden_states)\n\u001B[1;32m    352\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation_fn(hidden_states)\n\u001B[0;32m--> 353\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[0;32m~/anaconda3/envs/CLIP/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/CLIP/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.74 GiB total capacity; 14.51 GiB already allocated; 25.44 MiB free; 15.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "CLIP_feature = None\n",
    "for i in dataset_loader:\n",
    "    a = model.get_image_features(pixel_values = i.cuda()).float()\n",
    "    a = torch.hstack((a,a))\n",
    "    if CLIP_feature is None:\n",
    "        CLIP_feature = a\n",
    "    else:\n",
    "        CLIP_feature = torch.vstack((CLIP_feature,a))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T09:20:41.594309408Z",
     "start_time": "2023-10-05T09:20:40.662779492Z"
    }
   },
   "id": "ddaa400fd44c17e5"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "for i in dataset_loader_test:\n",
    "    image_features_test = model.get_image_features(pixel_values = i.cuda()).float()\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T07:37:53.022726690Z",
     "start_time": "2023-10-05T07:37:52.975240079Z"
    }
   },
   "id": "934e83c02460a14d"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.5236, -0.1173, -0.3356,  ...,  0.4854,  0.2284, -0.0049],\n        [ 0.5388,  0.2995, -0.1291,  ...,  0.8314,  0.0735,  0.0776],\n        [ 0.5322,  0.5193, -0.0377,  ...,  0.4968, -0.3671,  0.4157],\n        ...,\n        [ 0.2203,  0.3977, -0.3401,  ...,  0.5126, -0.1898,  0.3034],\n        [ 0.2498,  0.0956,  0.1781,  ..., -0.0081,  0.0991, -0.0192],\n        [ 0.0015,  0.1151, -0.0644,  ...,  0.6470,  0.2389,  0.2501]],\n       device='cuda:0', grad_fn=<MmBackward0>)"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T07:37:57.525911188Z",
     "start_time": "2023-10-05T07:37:57.518565118Z"
    }
   },
   "id": "d288da77aa040b01"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "image_features = image_features.detach().cpu().numpy()\n",
    "image_features_test = image_features_test.detach().cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T07:37:58.448702731Z",
     "start_time": "2023-10-05T07:37:58.443040221Z"
    }
   },
   "id": "af96091cfbe80a9d"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "features = np.vstack((image_features,image_features_test))\n",
    "a = TSNE().fit_transform(features)\n",
    "b = TSNE().fit_transform(image_features_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T07:38:01.852758156Z",
     "start_time": "2023-10-05T07:38:01.552822247Z"
    }
   },
   "id": "26ba417f39eeb021"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x7f2252903790>,\n <matplotlib.lines.Line2D at 0x7f2252903970>]"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(a[:64,0],a[:64,1],'+')\n",
    "plt.plot(a[64:,0],a[64:,1],'+','r')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T07:38:19.370858829Z",
     "start_time": "2023-10-05T07:38:19.320182656Z"
    }
   },
   "id": "9acb9b796e345aa6"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4fe3d44dd0f5ebe4"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "import random\n",
    "from matplotlib import animation\n",
    "from torchvision import transforms\n",
    "import ddpm.dataset\n",
    "from ddpm.helpers import gridify_output, load_parameters"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T09:13:53.423180922Z",
     "start_time": "2023-10-05T09:13:53.381805124Z"
    }
   },
   "id": "ade27b0b2e71bf09"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T09:10:51.875270060Z",
     "start_time": "2023-10-05T09:10:51.832548810Z"
    }
   },
   "id": "9f0af14fd15af216"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8a8eabaa9e465686"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
